spring.application.name=backend

# Import local .env for property overrides (optional)
# Supports running from project root or backend folder
# Explicitly treat .env files as .properties for Spring to parse
spring.config.import=optional:file:./backend/.env[.properties]

# Server port (override with env var SERVER_PORT)
server.port=${SERVER_PORT:8081}

# Database Configuration
spring.datasource.url=${DB_URL:jdbc:postgresql://localhost:5432/kortex}
spring.datasource.username=${DB_USERNAME:postgres}
spring.datasource.password=${DB_PASSWORD:root}
spring.datasource.driver-class-name=org.postgresql.Driver

# JPA/Hibernate Configuration
spring.jpa.hibernate.ddl-auto=update
spring.jpa.show-sql=true
spring.jpa.properties.hibernate.dialect=org.hibernate.dialect.PostgreSQLDialect
spring.jpa.properties.hibernate.format_sql=true

# JWT Configuration
jwt.secret=${JWT_SECRET:404E635266556A586E3272357538782F413F4428472B4B6250645367566B5970404E635266556A586E3272357538782F413F4428472B4B6250645367566B5970}
jwt.expiration=${JWT_EXPIRATION:86400000}

# Document Storage Configuration
document.storage.path=${DOCUMENT_STORAGE_PATH:./uploads}

# HuggingFace Configuration
# Prefer direct Inference API to avoid router 404s
huggingface.api.url=${HUGGINGFACE_API_URL:https://api-inference.huggingface.co/models}
huggingface.api.token=${HUGGINGFACE_API_TOKEN:your_huggingface_token_here}
huggingface.model.name=${HUGGINGFACE_MODEL:BAAI/bge-small-en-v1.5}
huggingface.embeddings.url=${HUGGINGFACE_EMBEDDINGS_URL:}
huggingface.embedding.dimension=384
huggingface.batch.size=32

# Qdrant Configuration
qdrant.host=${QDRANT_HOST:localhost}
qdrant.port=${QDRANT_PORT:6334}
qdrant.api.key=${QDRANT_API_KEY:}
qdrant.use.tls=${QDRANT_USE_TLS:false}
qdrant.collection.prefix=kortex

# RAG (Retrieval-Augmented Generation) Configuration
# Maximum tokens to include in LLM context (adjust based on your LLM's context window)
rag.max.context.tokens=3000
# Maximum number of chunks to retrieve before filtering by token limit
rag.context.chunk.limit=10
# Token estimation ratio (average tokens per word for English text)
rag.tokens.per.word=1.3

# LLM Configuration
# Provider: openai or groq
llm.provider=${LLM_PROVIDER:groq}
# OpenAI Configuration
llm.openai.api.key=${OPENAI_API_KEY:your_openai_api_key_here}
llm.openai.model=${OPENAI_MODEL:gpt-3.5-turbo}
llm.openai.max.tokens=1000
llm.openai.temperature=0.7
# Groq Configuration
llm.groq.api.key=${GROQ_API_KEY:your_groq_api_key_here}
llm.groq.api.url=${GROQ_API_URL:https://api.groq.com/openai/v1}
llm.groq.model=${GROQ_MODEL:llama-3.3-70b-versatile}
llm.groq.max.tokens=1000
llm.groq.temperature=0.7

# Summarization Configuration
# Maximum length of input text for summarization (in characters)
summarization.max.input.length=10000
# Maximum tokens for summary generation
summarization.max.tokens=300
# Temperature for summary generation (lower = more focused, higher = more creative)
summarization.temperature=0.3

# Tagging Configuration
# Maximum length of input text for tagging (in characters)
tagging.max.input.length=8000
# Maximum number of tags to extract per document
tagging.max.tags=10
# Maximum tokens for tag generation
tagging.max.tokens=200
# Temperature for tag extraction (lower = more consistent)
tagging.temperature=0.3
